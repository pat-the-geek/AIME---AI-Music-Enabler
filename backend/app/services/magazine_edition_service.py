"""
Service de gestion des √©ditions pr√©-g√©n√©r√©es de magazines.
Permet la g√©n√©ration, le stockage et la r√©cup√©ration d'√©ditions de magazines.
"""

import json
import os
import asyncio
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Any, Optional
import random
from sqlalchemy.orm import Session

from app.services.magazine_generator_service import MagazineGeneratorService
from app.services.ai_service import AIService
from app.core.config import get_settings

logger = logging.getLogger(__name__)


class MagazineEditionService:
    """Service pour g√©rer les √©ditions pr√©-g√©n√©r√©es de magazines."""
    
    def __init__(self, db: Session):
        self.db = db
        
        # Initialiser l'AI Service pour le MagazineGeneratorService
        settings = get_settings()
        secrets = settings.secrets
        euria_config = secrets.get('euria', {})
        
        self.ai_service = AIService(
            url=euria_config.get('url'),
            bearer=euria_config.get('bearer')
        )
        
        self.magazine_service = MagazineGeneratorService(db, self.ai_service)
        self.base_path = Path(__file__).parent.parent.parent.parent / "data" / "magazine-editions"
        self.base_path.mkdir(parents=True, exist_ok=True)
        
    def _get_edition_path(self, edition_id: str) -> Path:
        """Obtient le chemin complet d'une √©dition."""
        date_str = edition_id.split('-')[0:3]  # ['2026', '02', '03']
        date_folder = '-'.join(date_str)  # '2026-02-03'
        folder = self.base_path / date_folder
        folder.mkdir(parents=True, exist_ok=True)
        return folder / f"{edition_id}.json"
    
    def _generate_edition_id(self, edition_number: int) -> str:
        """G√©n√®re un ID d'√©dition unique."""
        now = datetime.now(timezone.utc)
        return f"{now.strftime('%Y-%m-%d')}-{edition_number:03d}"
    
    async def generate_edition(self, edition_number: int = 1) -> Dict[str, Any]:
        """
        G√©n√®re une nouvelle √©dition de magazine.
        
        Args:
            edition_number: Num√©ro de l'√©dition du jour (1-10)
            
        Returns:
            Dict contenant l'√©dition compl√®te
        """
        try:
            edition_id = self._generate_edition_id(edition_number)
            logger.info(f"üì∞ G√©n√©ration de l'√©dition {edition_id}...")
            
            # G√©n√©ration du magazine
            magazine_data = await self.magazine_service.generate_magazine()
            
            # Enrichissement complet (attendre que les descriptions soient enrichies)
            if magazine_data.get('enrichment_started'):
                logger.info(f"‚è≥ Attente de l'enrichissement pour l'√©dition {edition_id}...")
                # Attendre 3 minutes max pour les enrichissements (2-3 albums √ó 5-15s chacun)
                await asyncio.sleep(180)
                
                # IMPORTANT: Reg√©n√©rer le magazine pour r√©cup√©rer les descriptions enrichies depuis la DB
                logger.info(f"üîÑ Rechargement du magazine avec descriptions enrichies...")
                magazine_data = await self.magazine_service.generate_magazine()
            
            # Construction de l'√©dition
            edition = {
                'id': edition_id,
                'edition_number': edition_number,
                'generated_at': datetime.now(timezone.utc).isoformat(),
                'albums': magazine_data.get('albums', []),
                'pages': magazine_data.get('pages', []),
                'ai_layouts': magazine_data.get('ai_layouts', {}),
                'enrichment_completed': True,
                'version': '1.0'
            }
            
            # Sauvegarde
            self._save_edition(edition)
            
            logger.info(f"‚úÖ √âdition {edition_id} g√©n√©r√©e et sauvegard√©e ({len(edition['albums'])} albums)")
            return edition
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration de l'√©dition: {e}")
            raise
    
    def _save_edition(self, edition: Dict[str, Any]) -> None:
        """Sauvegarde une √©dition en JSON."""
        try:
            path = self._get_edition_path(edition['id'])
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(edition, f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ √âdition sauvegard√©e: {path}")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde de l'√©dition: {e}")
            raise
    
    def load_edition(self, edition_id: str) -> Optional[Dict[str, Any]]:
        """
        Charge une √©dition depuis le stockage.
        
        Args:
            edition_id: ID de l'√©dition √† charger
            
        Returns:
            Dict contenant l'√©dition ou None si non trouv√©e
        """
        try:
            path = self._get_edition_path(edition_id)
            if not path.exists():
                logger.warning(f"‚ö†Ô∏è √âdition non trouv√©e: {edition_id}")
                return None
            
            with open(path, 'r', encoding='utf-8') as f:
                edition = json.load(f)
            
            logger.info(f"üìñ √âdition charg√©e: {edition_id}")
            return edition
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de l'√©dition {edition_id}: {e}")
            return None
    
    def list_editions(self, limit: int = 50) -> List[Dict[str, Any]]:
        """
        Liste toutes les √©ditions disponibles.
        
        Args:
            limit: Nombre maximum d'√©ditions √† retourner
            
        Returns:
            Liste des m√©tadonn√©es des √©ditions (tri√©es par date d√©croissante)
        """
        try:
            editions = []
            
            # Parcourir tous les dossiers de dates
            for date_folder in sorted(self.base_path.iterdir(), reverse=True):
                if not date_folder.is_dir():
                    continue
                
                # Parcourir tous les fichiers JSON dans le dossier
                for edition_file in sorted(date_folder.glob("*.json"), reverse=True):
                    try:
                        with open(edition_file, 'r', encoding='utf-8') as f:
                            edition = json.load(f)
                        
                        # Extraire les m√©tadonn√©es
                        editions.append({
                            'id': edition['id'],
                            'edition_number': edition.get('edition_number', 1),
                            'generated_at': edition['generated_at'],
                            'album_count': len(edition.get('albums', [])),
                            'page_count': len(edition.get('pages', [])),
                            'enrichment_completed': edition.get('enrichment_completed', False)
                        })
                        
                        if len(editions) >= limit:
                            break
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erreur lors de la lecture de {edition_file}: {e}")
                        continue
                
                if len(editions) >= limit:
                    break
            
            logger.info(f"üìö {len(editions)} √©ditions list√©es")
            return editions
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du listage des √©ditions: {e}")
            return []
    
    def get_random_edition(self) -> Optional[Dict[str, Any]]:
        """
        R√©cup√®re une √©dition al√©atoire parmi les disponibles.
        
        Returns:
            Dict contenant l'√©dition ou None si aucune disponible
        """
        try:
            editions = self.list_editions()
            if not editions:
                logger.warning("‚ö†Ô∏è Aucune √©dition disponible")
                return None
            
            # S√©lection al√©atoire
            random_edition_meta = random.choice(editions)
            edition = self.load_edition(random_edition_meta['id'])
            
            logger.info(f"üé≤ √âdition al√©atoire s√©lectionn√©e: {random_edition_meta['id']}")
            return edition
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la s√©lection al√©atoire: {e}")
            return None
    
    async def generate_daily_batch(self, count: int = 10, delay_minutes: int = 30) -> List[str]:
        """
        G√©n√®re un lot d'√©ditions quotidiennes.
        
        Args:
            count: Nombre d'√©ditions √† g√©n√©rer
            delay_minutes: D√©lai entre chaque g√©n√©ration (en minutes)
            
        Returns:
            Liste des IDs des √©ditions g√©n√©r√©es
        """
        generated_ids = []
        
        try:
            logger.info(f"üöÄ D√©but de la g√©n√©ration de {count} √©ditions (intervalle: {delay_minutes}min)")
            
            for i in range(1, count + 1):
                try:
                    edition = await self.generate_edition(edition_number=i)
                    generated_ids.append(edition['id'])
                    
                    # D√©lai entre les g√©n√©rations (sauf pour la derni√®re)
                    if i < count:
                        logger.info(f"‚è∏Ô∏è Pause de {delay_minutes} minutes avant la prochaine g√©n√©ration...")
                        await asyncio.sleep(delay_minutes * 60)
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur lors de la g√©n√©ration de l'√©dition #{i}: {e}")
                    continue
            
            logger.info(f"‚úÖ G√©n√©ration de lot termin√©e: {len(generated_ids)}/{count} √©ditions cr√©√©es")
            return generated_ids
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration du lot: {e}")
            return generated_ids
    
    def cleanup_old_editions(self, keep_days: int = 30) -> int:
        """
        Supprime les √©ditions plus anciennes que keep_days jours.
        
        Args:
            keep_days: Nombre de jours d'√©ditions √† conserver
            
        Returns:
            Nombre d'√©ditions supprim√©es
        """
        try:
            deleted_count = 0
            cutoff_date = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
            cutoff_date = cutoff_date.replace(day=cutoff_date.day - keep_days)
            
            for date_folder in self.base_path.iterdir():
                if not date_folder.is_dir():
                    continue
                
                # Parser la date du dossier
                try:
                    folder_date = datetime.strptime(date_folder.name, '%Y-%m-%d')
                    folder_date = folder_date.replace(tzinfo=timezone.utc)
                    
                    if folder_date < cutoff_date:
                        # Supprimer tous les fichiers du dossier
                        for edition_file in date_folder.glob("*.json"):
                            edition_file.unlink()
                            deleted_count += 1
                        
                        # Supprimer le dossier vide
                        date_folder.rmdir()
                        logger.info(f"üóëÔ∏è Dossier supprim√©: {date_folder.name}")
                        
                except ValueError:
                    logger.warning(f"‚ö†Ô∏è Format de dossier invalide: {date_folder.name}")
                    continue
            
            logger.info(f"üßπ Nettoyage termin√©: {deleted_count} √©ditions supprim√©es")
            return deleted_count
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du nettoyage: {e}")
            return 0
    
    def cleanup_excess_editions(self, max_editions: int = 100) -> int:
        """
        Supprime les √©ditions les plus anciennes si le nombre total d√©passe max_editions.
        
        Args:
            max_editions: Nombre maximum d'√©ditions √† conserver (d√©faut: 100)
            
        Returns:
            Nombre d'√©ditions supprim√©es
        """
        try:
            deleted_count = 0
            
            # R√©cup√©rer toutes les √©ditions avec leur date
            all_editions = []
            for date_folder in self.base_path.iterdir():
                if not date_folder.is_dir():
                    continue
                
                for edition_file in date_folder.glob("*.json"):
                    try:
                        with open(edition_file, 'r', encoding='utf-8') as f:
                            edition = json.load(f)
                        
                        all_editions.append({
                            'file_path': edition_file,
                            'generated_at': datetime.fromisoformat(edition['generated_at'].replace('Z', '+00:00'))
                        })
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erreur lecture {edition_file}: {e}")
                        continue
            
            # Si on d√©passe la limite, supprimer les plus anciennes
            if len(all_editions) > max_editions:
                # Trier par date (plus anciennes en premier)
                all_editions.sort(key=lambda x: x['generated_at'])
                
                # Supprimer les √©ditions en exc√®s
                editions_to_delete = all_editions[:len(all_editions) - max_editions]
                
                for edition in editions_to_delete:
                    try:
                        edition['file_path'].unlink()
                        deleted_count += 1
                        logger.info(f"üóëÔ∏è √âdition supprim√©e: {edition['file_path'].name}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erreur suppression {edition['file_path']}: {e}")
                
                # Nettoyer les dossiers vides
                for date_folder in self.base_path.iterdir():
                    if date_folder.is_dir() and not list(date_folder.glob("*.json")):
                        date_folder.rmdir()
                        logger.info(f"üóëÔ∏è Dossier vide supprim√©: {date_folder.name}")
                
                logger.info(f"üßπ Nettoyage exc√©dent termin√©: {deleted_count} √©ditions supprim√©es (limite: {max_editions})")
            else:
                logger.info(f"‚úÖ Nombre d'√©ditions OK: {len(all_editions)}/{max_editions}")
            
            return deleted_count
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du nettoyage exc√©dent: {e}")
            return 0
