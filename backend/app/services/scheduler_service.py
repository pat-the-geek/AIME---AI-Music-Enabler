"""Service de scheduler optimis√© par IA pour t√¢ches intelligentes."""
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from datetime import datetime, timedelta, timezone
from sqlalchemy.orm import Session
from collections import Counter
import logging
import os
import json
from io import StringIO

from app.database import SessionLocal
from app.services.external.ai_service import AIService
from app.services.spotify_service import SpotifyService
from app.services.markdown_export_service import MarkdownExportService
from app.services.magazine_edition_service import MagazineEditionService
from app.models import Album, Track, ListeningHistory, Metadata, ScheduledTaskExecution

logger = logging.getLogger(__name__)

# Map entre task IDs et noms affich√©s avec emojis
TASK_NAMES = {
    'daily_enrichment': 'üîÑ Enrichissement quotidien',
    'generate_haiku_scheduled': 'üéã G√©n√©ration de ha√Økus',
    'export_collection_markdown': 'üìù Export Markdown',
    'export_collection_json': 'üíæ Export JSON',
    'weekly_haiku': 'üéã Ha√Øku hebdomadaire',
    'monthly_analysis': 'üìä Analyse mensuelle',
    'optimize_ai_descriptions': 'ü§ñ Optimisation IA',
    'generate_magazine_editions': 'üì∞ G√©n√©ration de magazines',
    'sync_discogs_daily': 'üíø Sync Discogs'
}


class SchedulerService:
    """Scheduler intelligent avec optimisation par IA."""
    
    def __init__(self, config: dict):
        self.config = config
        self.scheduler = AsyncIOScheduler()
        self.is_running = False
        
        # Initialiser services
        euria_config = config.get('euria', {})
        spotify_config = config.get('spotify', {})
        
        self.ai = AIService(
            url=euria_config.get('url'),
            bearer=euria_config.get('bearer')
        )
        
        self.spotify = SpotifyService(
            client_id=spotify_config.get('client_id'),
            client_secret=spotify_config.get('client_secret')
        )
    
    async def start(self):
        """D√©marrer le scheduler."""
        if self.is_running:
            logger.info("üìÖ Scheduler d√©j√† en cours d'ex√©cution")
            return
        
        # T√¢che quotidienne : enrichir albums manquants
        self.scheduler.add_job(
            self._daily_enrichment,
            trigger=CronTrigger(hour=2, minute=0),  # 2h du matin
            id='daily_enrichment',
            replace_existing=True
        )
        
        # T√¢che quotidienne : g√©n√©rer haikus pour 5 albums random
        self.scheduler.add_job(
            self._generate_random_haikus,
            trigger=CronTrigger(hour=6, minute=0),  # 6h du matin
            id='generate_haiku_scheduled',
            replace_existing=True
        )
        
        # T√¢che quotidienne : exporter collection en markdown
        self.scheduler.add_job(
            self._export_collection_markdown,
            trigger=CronTrigger(hour=8, minute=0),  # 8h du matin
            id='export_collection_markdown',
            replace_existing=True
        )
        
        # T√¢che quotidienne : exporter collection en JSON
        self.scheduler.add_job(
            self._export_collection_json,
            trigger=CronTrigger(hour=10, minute=0),  # 10h du matin
            id='export_collection_json',
            replace_existing=True
        )
        
        # T√¢che hebdomadaire : g√©n√©rer ha√Økus
        self.scheduler.add_job(
            self._weekly_haiku,
            trigger=CronTrigger(day_of_week='sun', hour=20, minute=0),  # Dimanche 20h
            id='weekly_haiku',
            replace_existing=True
        )
        
        # T√¢che mensuelle : analyse patterns profonde
        self.scheduler.add_job(
            self._monthly_analysis,
            trigger=CronTrigger(day=1, hour=3, minute=0),  # 1er du mois 3h
            id='monthly_analysis',
            replace_existing=True
        )
        
        # T√¢che intelligente : optimiser descriptions AI
        self.scheduler.add_job(
            self._optimize_ai_descriptions,
            trigger=CronTrigger(hour='*/6'),  # Toutes les 6h
            id='optimize_ai_descriptions',
            replace_existing=True
        )
        
        # T√¢che quotidienne : g√©n√©rer lot de magazines pr√©-g√©n√©r√©s
        self.scheduler.add_job(
            self._generate_magazine_editions,
            trigger=CronTrigger(hour=3, minute=0),  # 3h du matin
            id='generate_magazine_editions',
            replace_existing=True
        )
        
        # T√¢che quotidienne : synchroniser collection Discogs
        self.scheduler.add_job(
            self._sync_discogs_daily,
            trigger=CronTrigger(hour=4, minute=0),  # 4h du matin
            id='sync_discogs_daily',
            replace_existing=True
        )
        
        self.scheduler.start()
        self.is_running = True
        logger.info("üìÖ Scheduler d√©marr√© avec t√¢ches optimis√©es")
    
    async def stop(self):
        """Arr√™ter le scheduler."""
        if not self.is_running:
            logger.info("üìÖ Scheduler n'est pas en cours d'ex√©cution")
            return
        
        self.scheduler.shutdown()
        self.is_running = False
        logger.info("üìÖ Scheduler arr√™t√©")
    
    def _record_execution(self, task_id: str, status: str = 'success', error: str = None):
        """Enregistrer l'ex√©cution d'une t√¢che en base de donn√©es."""
        db = SessionLocal()
        try:
            execution = db.query(ScheduledTaskExecution).filter_by(task_id=task_id).first()
            
            if execution is None:
                execution = ScheduledTaskExecution(task_id=task_id)
                db.add(execution)
            
            execution.task_name = TASK_NAMES.get(task_id, task_id)
            execution.last_executed = datetime.now(timezone.utc)
            execution.last_status = status
            execution.updated_at = datetime.now(timezone.utc)
            
            # Mettre √† jour next_run_time si la t√¢che est en cours d'ex√©cution
            if self.is_running:
                try:
                    job = self.scheduler.get_job(task_id)
                    if job and job.next_run_time:
                        execution.next_run_time = job.next_run_time
                except:
                    pass
            
            db.commit()
            logger.debug(f"‚úÖ Ex√©cution enregistr√©e: {task_id} ({status})")
        except Exception as e:
            logger.error(f"‚ùå Erreur enregistrement ex√©cution {task_id}: {e}")
            db.rollback()
        finally:
            db.close()
    
    def get_status(self) -> dict:
        """Obtenir le statut du scheduler avec les ex√©cutions depuis la DB."""
        jobs = []
        executions_cache = {}  # Cache pour les ex√©cutions
        
        if self.is_running:
            db = SessionLocal()
            try:
                # Charger toutes les ex√©cutions enregistr√©es
                executions = db.query(ScheduledTaskExecution).all()
                for ex in executions:
                    executions_cache[ex.task_id] = {
                        'last_executed': ex.last_executed.isoformat() if ex.last_executed else None,
                        'last_status': ex.last_status or 'pending'
                    }
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur chargement ex√©cutions: {e}")
            finally:
                db.close()
            
            try:
                for job in self.scheduler.get_jobs():
                    try:
                        execution = executions_cache.get(job.id, {})
                        jobs.append({
                            'id': job.id,
                            'name': TASK_NAMES.get(job.id, job.name or job.id),
                            'next_run': job.next_run_time.isoformat() if job.next_run_time else None,
                            'last_execution': execution.get('last_executed'),
                            'last_status': execution.get('last_status', 'pending')
                        })
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erreur traitement job {getattr(job, 'id', 'unknown')}: {e}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur obtention jobs scheduler: {e}")
        
        return {
            'running': self.is_running,
            'jobs': jobs,
            'job_count': len(jobs)
        }
    
    async def _daily_enrichment(self):
        """Enrichissement quotidien automatique."""
        logger.info("üîÑ D√©but enrichissement quotidien")
        db = SessionLocal()
        
        try:
            # Enrichir 50 albums sans URL Spotify ou ann√©e
            albums = db.query(Album).filter(
                (Album.spotify_url == None) | (Album.year == None)
            ).limit(50).all()
            
            enriched = 0
            for album in albums:
                try:
                    artist_name = album.artists[0].name if album.artists else ""
                    
                    spotify_details = await self.spotify.search_album_details(artist_name, album.title)
                    if spotify_details:
                        if not album.spotify_url and spotify_details.get('spotify_url'):
                            album.spotify_url = spotify_details['spotify_url']
                        if not album.year and spotify_details.get('year'):
                            album.year = spotify_details['year']
                        enriched += 1
                        db.commit()
                except Exception as e:
                    logger.error(f"Erreur enrichissement {album.title}: {e}")
                    db.rollback()
                    continue
            
            logger.info(f"‚úÖ Enrichissement quotidien termin√©: {enriched} albums")
            self._record_execution('daily_enrichment', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur enrichissement quotidien: {e}")
            self._record_execution('daily_enrichment', 'error', str(e))
        finally:
            db.close()
    
    async def _weekly_haiku(self):
        """G√©n√©ration hebdomadaire de ha√Øku."""
        logger.info("üéã G√©n√©ration ha√Øku hebdomadaire")
        db = SessionLocal()
        
        try:
            # Analyser les 7 derniers jours
            seven_days_ago = int((datetime.now() - timedelta(days=7)).timestamp())
            recent_history = db.query(ListeningHistory).filter(
                ListeningHistory.timestamp >= seven_days_ago
            ).all()
            
            if not recent_history:
                logger.info("Pas d'historique r√©cent pour le ha√Øku")
                return
            
            # Extraire donn√©es
            artists = Counter()
            albums = Counter()
            
            for entry in recent_history:
                db_track = db.query(Track).get(entry.track_id)
                if db_track and db_track.album:
                    if db_track.album.artists:
                        for artist in db_track.album.artists:
                            artists[artist.name] += 1
                    albums[db_track.album.title] += 1
            
            listening_data = {
                'top_artists': [name for name, _ in artists.most_common(5)],
                'top_albums': [title for title, _ in albums.most_common(5)],
                'total_tracks': len(recent_history)
            }
            
            haiku = await self.ai.generate_haiku(listening_data)
            logger.info(f"üéã Ha√Øku g√©n√©r√©:\n{haiku}")
            self._record_execution('weekly_haiku', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration ha√Øku: {e}")
            self._record_execution('weekly_haiku', 'error', str(e))
        finally:
            db.close()
    
    async def _monthly_analysis(self):
        """Analyse mensuelle des patterns."""
        logger.info("üìä Analyse mensuelle des patterns")
        db = SessionLocal()
        
        try:
            # Analyser le mois pr√©c√©dent
            thirty_days_ago = int((datetime.now() - timedelta(days=30)).timestamp())
            monthly_history = db.query(ListeningHistory).filter(
                ListeningHistory.timestamp >= thirty_days_ago
            ).all()
            
            if not monthly_history:
                logger.info("Pas d'historique pour l'analyse mensuelle")
                return
            
            # Statistiques
            total_tracks = len(monthly_history)
            unique_days = len(set(
                datetime.fromtimestamp(e.timestamp).date() 
                for e in monthly_history
            ))
            avg_per_day = total_tracks / unique_days if unique_days > 0 else 0
            
            # Artistes top
            artists = Counter()
            for entry in monthly_history:
                db_track = db.query(Track).get(entry.track_id)
                if db_track and db_track.album and db_track.album.artists:
                    for artist in db_track.album.artists:
                        artists[artist.name] += 1
            
            top_artists = artists.most_common(10)
            
            logger.info(f"üìä Analyse mensuelle:")
            logger.info(f"  - Total √©coutes: {total_tracks}")
            logger.info(f"  - Jours actifs: {unique_days}")
            logger.info(f"  - Moyenne/jour: {avg_per_day:.1f}")
            logger.info(f"  - Top artiste: {top_artists[0] if top_artists else 'N/A'}")
            self._record_execution('monthly_analysis', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse mensuelle: {e}")
            self._record_execution('monthly_analysis', 'error', str(e))
        finally:
            db.close()
    
    async def _optimize_ai_descriptions(self):
        """Optimiser les descriptions IA des albums populaires."""
        logger.info("ü§ñ Optimisation descriptions IA")
        db = SessionLocal()
        
        try:
            # Trouver albums les plus √©cout√©s sans description IA
            from sqlalchemy import func
            
            popular_albums = db.query(
                Album.id,
                Album.title,
                func.count(ListeningHistory.id).label('play_count')
            ).join(Track).join(ListeningHistory).outerjoin(Metadata).filter(
                Metadata.ai_info == None
            ).group_by(Album.id).order_by(
                func.count(ListeningHistory.id).desc()
            ).limit(10).all()
            
            generated = 0
            for album_id, album_title, play_count in popular_albums:
                try:
                    album = db.query(Album).get(album_id)
                    if not album or not album.artists:
                        continue
                    
                    artist_name = album.artists[0].name
                    
                    # G√©n√©rer description
                    ai_info = await self.ai.generate_album_info(artist_name, album_title)
                    if ai_info:
                        metadata = Metadata(
                            album_id=album_id,
                            ai_info=ai_info
                        )
                        db.add(metadata)
                        db.commit()
                        generated += 1
                        logger.info(f"‚ú® Description IA ajout√©e: {album_title} ({play_count} √©coutes)")
                
                except Exception as e:
                    logger.error(f"Erreur description {album_title}: {e}")
                    db.rollback()
                    continue
            
            logger.info(f"ü§ñ Optimisation termin√©e: {generated} descriptions g√©n√©r√©es")
            self._record_execution('optimize_ai_descriptions', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur optimisation IA: {e}")
            self._record_execution('optimize_ai_descriptions', 'error', str(e))
        finally:
            db.close()
    
    async def _generate_random_haikus(self):
        """G√©n√©rer haikus pour 5 albums - Format IDENTIQUE √† l'API /collection/markdown/presentation."""
        import random
        
        logger.info("üéã G√©n√©ration haikus pour 5 albums random - Format API")
        db = SessionLocal()
        
        try:
            # R√©cup√©rer 5 albums al√©atoires
            all_albums = db.query(Album).filter(Album.source == 'discogs').all()
            if len(all_albums) < 5:
                logger.warning("Pas assez d'albums pour g√©n√©rer haikus")
                return
            
            selected_albums = random.sample(all_albums, 5)
            
            # G√©n√©rer markdown - Format IDENTIQUE √† l'API
            markdown = "# Album Ha√Øku\n"
            
            # Date du jour
            now = datetime.now()
            # Formater la date: "The 1 of February, 2026"
            day = now.strftime("%-d" if os.name != 'nt' else "%#d")  # Pas de z√©ro au jour
            month = now.strftime("%B")
            year = now.strftime("%Y")
            date_str = f"#### The {day} of {month}, {year}"
            markdown += f"{date_str}\n"
            markdown += f"\t\t{len(selected_albums)} albums from Discogs collection\n"
            
            # Ha√Øku global
            haiku_text = ""
            try:
                haiku_prompt = "G√©n√®re un ha√Øku court sur la musique et les albums. R√©ponds uniquement avec le ha√Øku en 3 lignes, sans num√©rotation."
                haiku_text = await self.ai.ask_for_ia(haiku_prompt, max_tokens=100)
                # Ajouter chaque ligne du ha√Øku avec indentation
                for line in haiku_text.strip().split('\n'):
                    markdown += f"\t\t{line.strip()}\n"
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur g√©n√©ration ha√Øku global: {e}")
                # Ha√Øku par d√©faut
                markdown += "\t\tMusique qui danse,\n"
                markdown += "\t\talbums en harmonie,\n"
                markdown += "\t\tc≈ìur qui s'envole.\n"
            
            markdown += "---\n"
            
            # G√©n√©rer une section pour chaque album
            for album in selected_albums:
                # Artiste en titre
                if album.artists:
                    artist_name = album.artists[0].name
                    markdown += f"# {artist_name}\n"
                
                # Titre, ann√©e et infos
                title_line = f"#### {album.title}"
                if album.year:
                    title_line += f" ({album.year})"
                markdown += f"{title_line}\n"
                
                # Liens Spotify et Discogs
                markdown += "\t###### üéß"
                if album.spotify_url:
                    markdown += f" [Listen with Spotify]({album.spotify_url})"
                markdown += "  üë•"
                if album.discogs_url:
                    markdown += f" [Read on Discogs]({album.discogs_url})"
                markdown += "\n\t###### üíø "
                markdown += f"{album.support if album.support else 'Digital'}\n"
                
                # Description g√©n√©r√©e par l'IA
                description = ""
                try:
                    album_lower = album.title.lower()
                    artist_lower = (album.artists[0].name.lower() if album.artists else "artiste inconnu")
                    description_prompt = f"""Pr√©sente moi l'album {album_lower} de {artist_lower}. 
N'ajoute pas de questions ou de commentaires. 
Limite ta r√©ponse √† 35 mots maximum.
R√©ponds uniquement en fran√ßais."""
                    description = await self.ai.ask_for_ia(description_prompt, max_tokens=100)
                    
                    # Fallback si pas de description
                    if not description or len(description) < 10:
                        description = f"Album {album.title} sorti en {album.year if album.year else '?'}. ≈íuvre musicale enrichissante, √† d√©couvrir absolument."
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur g√©n√©ration description pour {album.title}: {e}")
                    description = f"Album {album.title} sorti en {album.year if album.year else '?'}. ≈íuvre musicale enrichissante, √† d√©couvrir absolument."
                
                # Ajouter la description avec indentation
                description = description.strip()
                for line in description.split('\n'):
                    markdown += f"\t\t{line}\n"
                
                # Image HTML
                if album.images and album.images[0].url:
                    image_url = album.images[0].url
                    markdown += f"\n\n<img src='{image_url}' />\n"
                
                markdown += "---\n"
            
            # Footer
            markdown += "\t\tPython generated with love, for iA Presenter using Euria AI from Infomaniak\n"
            
            # Cr√©er chemin absolu pour le r√©pertoire de sortie
            current_dir = os.path.abspath(__file__)
            for _ in range(4):
                current_dir = os.path.dirname(current_dir)
            project_root = current_dir
            output_dir = os.path.join(project_root, self.config.get('scheduler', {}).get('output_dir', 'Scheduled Output'))
            
            os.makedirs(output_dir, exist_ok=True)
            
            # G√©n√©rer nom fichier avec date/heure
            timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
            filename = f"generate-haiku-{timestamp}.md"
            filepath = os.path.join(output_dir, filename)
            
            # Sauvegarder fichier
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown)
            
            logger.info(f"‚úÖ Haikus sauvegard√©s: {filepath}")
            logger.info(f"üìÑ Format: Album Ha√Øku (identique √† API)")
            
            # Nettoyer les anciens fichiers
            self._cleanup_old_files()
            self._record_execution('generate_haiku_scheduled', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration haikus: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self._record_execution('generate_haiku_scheduled', 'error', str(e))
        finally:
            db.close()
    
    async def _export_collection_markdown(self):
        """Exporter la collection compl√®te en markdown avec le m√™me format que l'API."""
        logger.info("üìù Export collection en markdown")
        db = SessionLocal()
        
        try:
            # Utiliser le m√™me service que l'API pour garantir l'identit√© du format
            markdown_content = MarkdownExportService.get_collection_markdown(db)
            
            if not markdown_content:
                logger.warning("Aucun album √† exporter")
                return
            
            # Cr√©er chemin absolu pour le r√©pertoire de sortie
            current_dir = os.path.abspath(__file__)
            for _ in range(4):
                current_dir = os.path.dirname(current_dir)
            project_root = current_dir
            output_dir = os.path.join(project_root, self.config.get('scheduler', {}).get('output_dir', 'Scheduled Output'))
            
            os.makedirs(output_dir, exist_ok=True)
            
            # G√©n√©rer nom fichier avec date/heure
            timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
            filename = f"export-markdown-{timestamp}.md"
            filepath = os.path.join(output_dir, filename)
            
            # Sauvegarder fichier
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            logger.info(f"‚úÖ Collection markdown sauvegard√©e: {filepath}")
            
            # Nettoyer les anciens fichiers
            self._cleanup_old_files()
            self._record_execution('export_collection_markdown', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur export markdown: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self._record_execution('export_collection_markdown', 'error', str(e))
        finally:
            db.close()
    
    async def _export_collection_json(self):
        """Exporter la collection compl√®te en JSON avec le m√™me format que l'API."""
        logger.info("üìä Export collection en JSON")
        db = SessionLocal()
        
        try:
            # R√©cup√©rer tous les albums de collection Discogs, tri√©s par titre
            albums = db.query(Album).filter(Album.source == 'discogs').order_by(Album.title).all()
            
            if not albums:
                logger.warning("Aucun album √† exporter")
                return
            
            # Construire les donn√©es JSON avec le m√™me format que l'API
            data = {
                "export_date": datetime.now().isoformat(),
                "total_albums": len(albums),
                "albums": []
            }
            
            for album in albums:
                # Traiter les images
                images = []
                if album.images:
                    for img in album.images:
                        images.append({
                            "url": img.url,
                            "type": img.image_type,
                            "source": img.source
                        })
                
                # Traiter les m√©tadonn√©es
                metadata = {}
                if album.album_metadata:
                    meta = album.album_metadata
                    metadata = {
                        "ai_info": meta.ai_info,
                        "resume": meta.resume,
                        "labels": meta.labels,
                        "film_title": meta.film_title,
                        "film_year": meta.film_year,
                        "film_director": meta.film_director
                    }
                
                album_data = {
                    "id": album.id,
                    "title": album.title,
                    "artists": [artist.name for artist in album.artists],
                    "year": album.year,
                    "support": album.support,
                    "discogs_id": album.discogs_id,
                    "spotify_url": album.spotify_url,
                    "discogs_url": album.discogs_url,
                    "images": images,
                    "created_at": album.created_at.isoformat() if album.created_at else None,
                    "metadata": metadata
                }
                
                data["albums"].append(album_data)
            
            # Cr√©er chemin absolu pour le r√©pertoire de sortie
            current_dir = os.path.abspath(__file__)
            for _ in range(4):
                current_dir = os.path.dirname(current_dir)
            project_root = current_dir
            output_dir = os.path.join(project_root, self.config.get('scheduler', {}).get('output_dir', 'Scheduled Output'))
            
            os.makedirs(output_dir, exist_ok=True)
            
            # G√©n√©rer nom fichier avec date/heure
            timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
            filename = f"export-json-{timestamp}.json"
            filepath = os.path.join(output_dir, filename)
            
            # Sauvegarder fichier
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ Collection JSON sauvegard√©e: {filepath}")
            
            # Nettoyer les anciens fichiers
            self._cleanup_old_files()
            self._record_execution('export_collection_json', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur export JSON: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self._record_execution('export_collection_json', 'error', str(e))
        finally:
            db.close()
    
    def _cleanup_old_files(self):
        """Nettoyer les anciens fichiers en conservant seulement les N derniers de chaque type."""
        import glob
        
        max_files = self.config.get('scheduler', {}).get('max_files_per_type', 5)
        
        # Calculer le chemin du r√©pertoire de sortie
        current_dir = os.path.abspath(__file__)
        for _ in range(4):
            current_dir = os.path.dirname(current_dir)
        project_root = current_dir
        output_dir = os.path.join(project_root, self.config.get('scheduler', {}).get('output_dir', 'Scheduled Output'))
        
        if not os.path.exists(output_dir):
            return
        
        # D√©finir les patterns pour chaque type de fichier
        file_patterns = {
            'generate-haiku-*.md': 'haiku',
            'export-markdown-*.md': 'markdown',
            'export-json-*.json': 'json'
        }
        
        for pattern, file_type in file_patterns.items():
            files = glob.glob(os.path.join(output_dir, pattern))
            
            if len(files) > max_files:
                # Trier par date de modification (les plus anciens en premier)
                files_sorted = sorted(files, key=lambda x: os.path.getmtime(x))
                
                # Supprimer les fichiers en exc√®s (garder seulement les max_files les plus r√©cents)
                files_to_delete = files_sorted[:-max_files]
                
                for file_path in files_to_delete:
                    try:
                        os.remove(file_path)
                        logger.info(f"üóëÔ∏è Supprim√© fichier ancien ({file_type}): {os.path.basename(file_path)}")
                    except Exception as e:
                        logger.error(f"‚ùå Erreur suppression {file_path}: {e}")
    
    async def enrich_imported_albums(self, albums_to_enrich: dict) -> dict:
        """Enrichir les albums import√©s en arri√®re-plan (Spotify + IA)."""
        self.last_executions['enrich_imported_albums'] = datetime.now(timezone.utc).isoformat()
        logger.info(f"üé® Enrichissement de {len(albums_to_enrich)} albums import√©s (en arri√®re-plan)")
        db = SessionLocal()
        
        enriched_count = 0
        total_albums = len(albums_to_enrich)
        
        try:
            for album_index, album_info in enumerate(albums_to_enrich.values(), 1):
                try:
                    album_id = album_info['album_id']
                    artist = album_info['artist']
                    title = album_info['title']
                    
                    logger.info(f"üé® Enrichissement album {album_index}/{total_albums}: {artist} - {title}")
                    
                    album = db.query(Album).filter_by(id=album_id).first()
                    if not album:
                        continue
                    
                    # Enrichir Spotify
                    if not album.spotify_url:
                        try:
                            spotify_url = await self.spotify.search_album_url(artist, title)
                            if spotify_url:
                                album.spotify_url = spotify_url
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erreur Spotify pour {title}: {e}")
                    
                    # Images Spotify
                    if not any(img.source == 'spotify' for img in album.images):
                        try:
                            from app.models import Image
                            album_image = await self.spotify.search_album_image(artist, title)
                            if album_image:
                                img = Image(
                                    url=album_image,
                                    image_type='album',
                                    source='spotify',
                                    album_id=album.id
                                )
                                db.add(img)
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erreur image Spotify pour {title}: {e}")
                    
                    # Images Last.fm (appel direct HTTP)
                    if not any(img.source == 'lastfm' for img in album.images):
                        try:
                            from app.services.lastfm_service import LastFMService
                            from config.settings import get_settings
                            settings = get_settings()
                            secrets = settings.secrets
                            lastfm_config = secrets.get('lastfm', {})
                            lastfm_service = LastFMService(
                                api_key=lastfm_config.get('api_key'),
                                api_secret=lastfm_config.get('api_secret'),
                                username=lastfm_config.get('username')
                            )
                            lastfm_image = await lastfm_service.get_album_image(artist, title)
                            if lastfm_image:
                                from app.models import Image
                                img = Image(
                                    url=lastfm_image,
                                    image_type='album',
                                    source='lastfm',
                                    album_id=album.id
                                )
                                db.add(img)
                                logger.info(f"‚úÖ Image Last.fm ajout√©e pour {artist} - {title}")
                        except Exception as e:
                            logger.error(f"‚ùå Erreur image Last.fm pour {artist} - {title}: {e}")
                    
                    # Description IA
                    if not album.album_metadata or not album.album_metadata.ai_info:
                        try:
                            # D√©lai pour ne pas saturer l'API IA
                            import asyncio
                            await asyncio.sleep(1.0)
                            ai_info = await self.ai.generate_album_info(artist, title)
                            if ai_info:
                                if not album.album_metadata:
                                    metadata = Metadata(album_id=album.id, ai_info=ai_info)
                                    db.add(metadata)
                                else:
                                    album.album_metadata.ai_info = ai_info
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erreur IA pour {title}: {e}")
                    
                    enriched_count += 1
                    if enriched_count % 10 == 0:
                        db.commit()
                        logger.info(f"üíæ {enriched_count}/{total_albums} albums enrichis...")
                    if enriched_count % 50 == 0:
                        db.flush()  # Flush plus souvent pour √©viter les locks
                        
                except Exception as e:
                    logger.error(f"‚ùå Erreur enrichissement album: {e}")
                    db.rollback()
                    continue
            
            db.commit()
            logger.info(f"‚úÖ Enrichissement d'import termin√©: {enriched_count} albums enrichis")
            
            return {
                'status': 'completed',
                'albums_enriched': enriched_count,
                'total_albums': total_albums
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur enrichissement import: {e}")
            db.rollback()
            return {
                'status': 'error',
                'error': str(e),
                'albums_enriched': enriched_count,
                'total_albums': total_albums
            }
        finally:
            db.close()
    
    async def _sync_discogs_daily(self):
        """Synchronisation quotidienne de la collection Discogs."""
        logger.info("üíø D√©but synchronisation quotidienne Discogs")
        
        try:
            # Importer la fonction de sync depuis le API
            from app.api.v1.services import _sync_discogs_task
            
            # Ex√©cuter la sync
            await _sync_discogs_task(limit=None)
            
            logger.info("‚úÖ Synchronisation Discogs quotidienne termin√©e")
            self._record_execution('sync_discogs_daily', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sync Discogs quotidienne: {e}")
            self._record_execution('sync_discogs_daily', 'error', str(e))
    
    async def trigger_task(self, task_name: str) -> dict:
        """D√©clencher manuellement une t√¢che."""
        tasks = {
            'daily_enrichment': self._daily_enrichment,
            'generate_haiku_scheduled': self._generate_random_haikus,
            'export_collection_markdown': self._export_collection_markdown,
            'export_collection_json': self._export_collection_json,
            'weekly_haiku': self._weekly_haiku,
            'monthly_analysis': self._monthly_analysis,
            'optimize_ai_descriptions': self._optimize_ai_descriptions,
            'generate_magazine_editions': self._generate_magazine_editions,
            'sync_discogs_daily': self._sync_discogs_daily
        }
        
        if task_name not in tasks:
            raise ValueError(f"T√¢che inconnue: {task_name}")
        
        logger.info(f"üöÄ D√©clenchement manuel: {task_name}")
        await tasks[task_name]()
        
        return {
            'task': task_name,
            'status': 'completed',
            'timestamp': datetime.now().isoformat()
        }
    
    async def _generate_magazine_editions(self):
        """G√©n√©ration quotidienne de magazines pr√©-g√©n√©r√©s."""
        logger.info("üì∞ D√©but g√©n√©ration lot de magazines")
        db = SessionLocal()
        
        try:
            edition_service = MagazineEditionService(db)
            
            # G√©n√©rer 10 √©ditions avec 30 minutes d'intervalle
            generated_ids = await edition_service.generate_daily_batch(count=10, delay_minutes=30)
            
            # Nettoyer les √©ditions de plus de 30 jours
            deleted_count = edition_service.cleanup_old_editions(keep_days=30)
            
            # Nettoyer l'exc√©dent si > 100 √©ditions
            excess_deleted = edition_service.cleanup_excess_editions(max_editions=100)
            
            logger.info(f"‚úÖ G√©n√©ration magazines termin√©e: {len(generated_ids)} cr√©√©es, {deleted_count} anciennes supprim√©es, {excess_deleted} exc√©dent supprim√©")
            self._record_execution('generate_magazine_editions', 'success')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration magazines: {e}")
            self._record_execution('generate_magazine_editions', 'error', str(e))
        finally:
            db.close()
