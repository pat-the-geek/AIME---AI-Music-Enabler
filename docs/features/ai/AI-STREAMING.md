# ğŸŒŠ Streaming AI - Portrait d'Artiste

**Date:** 4 fÃ©vrier 2026  
**Version:** 1.0.0

---

## ğŸ“‹ Vue d'Ensemble

Le **streaming AI** permet d'afficher le texte du portrait d'artiste **caractÃ¨re par caractÃ¨re** au fur et Ã  mesure de la gÃ©nÃ©ration par l'IA, au lieu d'attendre la rÃ©ponse complÃ¨te.

### âœ¨ Avantages

| Aspect | Sans Streaming | Avec Streaming |
|--------|----------------|----------------|
| **Temps d'attente perÃ§u** | 1-2 minutes | âš¡ ImmÃ©diat |
| **Feedback utilisateur** | â³ Loading spinner | âœ… Texte en temps rÃ©el |
| **ExpÃ©rience** | Attente passive | ğŸ“– Lecture progressive |
| **Annulation** | âŒ Impossible | âœ… Possible |
| **Engagement** | Faible | ğŸ¯ Ã‰levÃ© |

---

## ğŸ—ï¸ Architecture

### Backend

```
Client (Browser)
    â†“ HTTP GET /api/v1/collection/artists/{id}/article/stream
    â†“
FastAPI Endpoint
    â†“ StreamingResponse (SSE)
    â†“
ArtistArticleService.generate_article_stream()
    â†“ Generator async
    â†“
AIService.ask_for_ia_stream()
    â†“ httpx.stream() + yield chunks
    â†“
EurIA API (Infomaniak AI - Mistral3)
    â†“ stream: true
    â†“
data: chunk1\n\n
data: chunk2\n\n
data: chunk3\n\n
...
```

### Frontend

```
User clicks "GÃ©nÃ©rer"
    â†“
fetch('/article/stream', { Accept: 'text/event-stream' })
    â†“
response.body.getReader()
    â†“
while (!done) {
    â†“ read() â†’ decode() â†’ parse SSE
    â†“
    setStreamedContent(prev => prev + chunk)
    â†“ React re-render
    â†“ ReactMarkdown displays
}
```

---

## ğŸ”§ ImplÃ©mentation Technique

### 1. Backend - AIService

**Fichier:** `backend/app/services/ai_service.py`

```python
async def ask_for_ia_stream(self, prompt: str, max_tokens: int = 500):
    """Streaming avec Server-Sent Events (SSE)."""
    
    payload = {
        "model": "mistral3",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": 0.7,
        "stream": True  # â­ Activer le streaming
    }
    
    async with httpx.AsyncClient(timeout=120.0) as client:
        async with client.stream("POST", self.url, json=payload) as response:
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data = json.loads(line[6:])
                    content = data["choices"][0]["delta"]["content"]
                    yield f"data: {content}\n\n"  # Format SSE
```

**Points clÃ©s:**
- `stream: True` dans le payload API
- `client.stream()` au lieu de `client.post()`
- `yield` pour envoyer chunk par chunk
- Format SSE: `data: {content}\n\n`

### 2. Backend - Endpoint FastAPI

**Fichier:** `backend/app/api/v1/artists.py`

```python
@router.get("/{artist_id}/article/stream")
async def stream_artist_article(artist_id: int, db: Session = Depends(get_db)):
    """Streaming SSE du portrait d'artiste."""
    
    async def generate_stream():
        # 1. Envoyer mÃ©tadonnÃ©es
        metadata = {"type": "metadata", "artist_name": artist.name, ...}
        yield f"data: {json.dumps(metadata)}\n\n"
        
        # 2. Streamer le contenu
        async for chunk in article_service.generate_article_stream(artist_id):
            yield chunk
        
        # 3. Signal de fin
        yield f"data: {{\"type\": \"done\"}}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Important pour nginx
        }
    )
```

**Points clÃ©s:**
- `StreamingResponse` avec gÃ©nÃ©rateur async
- `media_type="text/event-stream"`
- Headers pour dÃ©sactiver le cache
- `X-Accel-Buffering: no` pour Ã©viter le buffering nginx

### 3. Frontend - Fetch Stream

**Fichier:** `frontend/src/pages/ArtistArticle.tsx`

```typescript
const handleGenerateArticleStream = async () => {
  const response = await fetch(`${baseURL}/collection/artists/${id}/article/stream`, {
    headers: { 'Accept': 'text/event-stream' }
  })
  
  const reader = response.body?.getReader()
  const decoder = new TextDecoder()
  let buffer = ''
  
  while (true) {
    const { done, value } = await reader.read()
    if (done) break
    
    // DÃ©coder et parser les Ã©vÃ©nements SSE
    buffer += decoder.decode(value, { stream: true })
    const lines = buffer.split('\n')
    buffer = lines.pop() || ''
    
    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6)
        
        try {
          const parsed = JSON.parse(data)
          if (parsed.type === 'metadata') {
            setStreamMetadata(parsed)
          } else if (parsed.type === 'done') {
            setIsStreaming(false)
          }
        } catch {
          // Texte brut - ajouter au contenu
          setStreamedContent(prev => prev + data)
        }
      }
    }
  }
}
```

**Points clÃ©s:**
- `response.body.getReader()` pour lire le stream
- `TextDecoder` pour dÃ©coder les bytes
- Buffer pour gÃ©rer les lignes incomplÃ¨tes
- Parse JSON pour mÃ©tadonnÃ©es, texte brut pour contenu
- `setStreamedContent(prev => prev + data)` pour accumulation

### 4. Frontend - Affichage Temps RÃ©el

```tsx
{streamedContent && (
  <Paper>
    <ReactMarkdown remarkPlugins={[remarkGfm]}>
      {streamedContent}
    </ReactMarkdown>
    
    {isStreaming && (
      <Box>
        <CircularProgress size={16} />
        <Typography>GÃ©nÃ©ration en cours...</Typography>
      </Box>
    )}
  </Paper>
)}
```

**Points clÃ©s:**
- ReactMarkdown re-render Ã  chaque update
- Indicateur de streaming en cours
- Affichage progressif du Markdown formatÃ©

---

## ğŸ“Š Format SSE (Server-Sent Events)

### Structure

```
data: texte chunk 1\n\n
data: texte chunk 2\n\n
data: {"type": "metadata", "artist_name": "Beatles"}\n\n
data: texte chunk 3\n\n
data: {"type": "done"}\n\n
```

### Types de Messages

| Type | Format | Usage |
|------|--------|-------|
| **Texte** | `data: contenu\n\n` | Chunks de texte Markdown |
| **MÃ©tadonnÃ©es** | `data: {"type":"metadata",...}\n\n` | Info artiste |
| **Fin** | `data: {"type":"done"}\n\n` | Signal fin de stream |
| **Erreur** | `data: {"type":"error","message":"..."}\n\n` | Erreur |

---

## ğŸ¯ Cas d'Usage

### 1. Portrait d'Artiste (3000 mots)

**Temps de gÃ©nÃ©ration:**
- Sans streaming: â³ 60-120 secondes d'attente
- Avec streaming: âš¡ Affichage dÃ¨s la 1Ã¨re seconde

**ExpÃ©rience utilisateur:**
```
0s   â†’ Clic "GÃ©nÃ©rer"
0.5s â†’ "# The Beatles : Portrait..." apparaÃ®t
1s   â†’ "## Introduction\n\nDans l'histoire..." s'affiche
2s   â†’ L'utilisateur commence Ã  lire
...
60s  â†’ Article complet affichÃ©
```

### 2. Avantages PerÃ§us

1. **RÃ©activitÃ©:** Feedback immÃ©diat au lieu d'attente passive
2. **Engagement:** L'utilisateur lit pendant la gÃ©nÃ©ration
3. **Transparence:** Voir l'IA "penser" en temps rÃ©el
4. **Annulation:** PossibilitÃ© de stopper si le contenu ne convient pas

---

## ğŸš€ Configuration EurIA

### ParamÃ¨tres API

```python
{
    "model": "mistral3",         # ModÃ¨le Mistral
    "stream": True,              # Activer streaming
    "max_tokens": 4000,          # ~3000 mots
    "temperature": 0.7,          # CrÃ©ativitÃ© modÃ©rÃ©e
    "messages": [...]
}
```

### Headers Requis

```python
{
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json"
}
```

### Timeout

- Client HTTP: `120.0` secondes
- Suffisant pour 3000 mots en streaming

---

## âš ï¸ Limitations & Contraintes

### 1. **Circuit Breaker**

Si le service EurIA est en `OPEN`:
```python
if ai_circuit_breaker.state == "OPEN":
    yield f"data: Service temporairement indisponible\n\n"
    return
```

### 2. **Nginx Buffering**

Sans `X-Accel-Buffering: no`, nginx peut buffer tout le stream et l'envoyer d'un coup.

### 3. **CORS**

Pour les requÃªtes cross-origin:
```python
headers={
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Methods": "GET",
}
```

### 4. **DÃ©connexion Client**

Si le client ferme la connexion:
```python
try:
    async for chunk in stream:
        yield chunk
except asyncio.CancelledError:
    logger.info("Client disconnected")
    return
```

---

## ğŸ“ˆ Performance

### MÃ©triques

| MÃ©trique | Valeur |
|----------|--------|
| **Premier chunk** | ~0.3-0.5s |
| **DÃ©bit** | ~50-100 tokens/s |
| **3000 mots** | ~60-90s total |
| **Bande passante** | ~1-2 KB/s |

### Comparaison

**Sans Streaming:**
- Charge serveur: 100% pendant 60s
- ExpÃ©rience utilisateur: â³ 60s attente â†’ ğŸ’¥ Tout d'un coup

**Avec Streaming:**
- Charge serveur: RÃ©partie sur 60s
- ExpÃ©rience utilisateur: âš¡ ImmÃ©diat â†’ ğŸ“– Lecture progressive

---

## ğŸ› Debugging

### Logs Backend

```python
logger.info(f"ğŸ“ Streaming article pour {artist.name}")
logger.info(f"âœ… Streaming terminÃ©")
```

### Console Frontend

```javascript
console.log('Chunk reÃ§u:', data)
console.log('Contenu actuel:', streamedContent)
```

### Tester avec cURL

```bash
curl -N http://localhost:8000/api/v1/collection/artists/123/article/stream
```

L'option `-N` dÃ©sactive le buffering cURL.

---

## ğŸ“ Ressources

### Documentation

- **Server-Sent Events (SSE):** [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- **httpx Streaming:** [HTTPX Docs](https://www.python-httpx.org/advanced/#streaming-responses)
- **FastAPI StreamingResponse:** [FastAPI Docs](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)

### Standards

- **SSE Spec:** [W3C](https://html.spec.whatwg.org/multipage/server-sent-events.html)
- **OpenAI Streaming:** [OpenAI API Docs](https://platform.openai.com/docs/api-reference/streaming)

---

## âœ… RÃ©sumÃ©

Le **streaming AI** transforme l'expÃ©rience utilisateur en passant d'une **attente passive** (60-120s) Ã  une **lecture progressive immÃ©diate** (<1s).

**Technos clÃ©s:**
- Backend: FastAPI `StreamingResponse` + httpx `stream()`
- API: EurIA Mistral3 avec `stream: true`
- Frontend: Fetch API `getReader()` + React state
- Format: Server-Sent Events (SSE)

**Impact:**
- âœ… Feedback immÃ©diat
- âœ… Meilleure perception de performance
- âœ… Engagement utilisateur accru
- âœ… PossibilitÃ© d'annulation

---

**Version:** 1.0.0  
**Auteur:** GitHub Copilot  
**Feature:** Portrait d'Artiste avec Streaming AI

ğŸŒŠ **Le futur, c'est maintenant.**
